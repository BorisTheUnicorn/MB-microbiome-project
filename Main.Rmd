---
title: "MB analysis setup and batch effect correction"
author: "Orr Tobaly"
date: "2025-03-29"
output: pdf_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Install core packages
```{r}
install.packages(c("vegan", "ggpubr", "patchwork"))

# Microbiome-specific packages
if (!requireNamespace("BiocManager", quietly = TRUE))
    install.packages("BiocManager")

BiocManager::install(c("microbiome", "phyloseq"))

# QIIME2R for importing QIIME2 files
install.packages("devtools")
devtools::install_github("jbisanz/qiime2R")

# microeco package
devtools::install_github("ChiLiubio/microeco")
```

# Loading packages
```{r}
library(vegan)
library(ggpubr)
library(microbiome)
library(phyloseq)
library(qiime2R)
library(microeco)
library(dplyr)
library(tidyverse)
library(magrittr)
library(sva)
library(GUniFrac)
```

# Batch effect analysis
```{r}
# Import data
metadata <- read_q2metadata("00.metadata.combined.2020_2021.tsv")
taxa <- read_qza("taxonomy-dada2.qza")$data %>% parse_taxonomy()
feature_table <- as.data.frame(read_qza("table-dada2.qza")$data)
tree <- read_qza("rooted-tree-dada2.qza")$data

# weird batch #4
# weird_samples <- c("622-Jacob006-Re2457-0", "622-Jacob007-Re2488-0")
# metadata <- metadata %>% filter(!(SampleID %in% weird_samples))
# feature_table <- feature_table %>% select(-any_of(weird_samples))
# dataset$tidy_dataset()

# Prepare the microeco dataset
rownames(metadata) <- metadata[,1]  # Set sample IDs as rownames
dataset <- microtable$new(sample_table = metadata, 
                          otu_table = feature_table, 
                          tax_table = taxa,
                          phylo_tree = tree)

# Basic data tidying
dataset$tidy_dataset()
dataset$tax_table %<>% base::subset(Kingdom == "Archaea" | Kingdom == "Bacteria")
dataset$filter_pollution(taxa = c("mitochondria", "chloroplast"))
dataset$rarefy_samples(sample.size = 5000, method = "rarefy")  # Rarefying to 5000 reads per sample

# Calculate beta diversity
dataset$cal_betadiv(unifrac = TRUE)

# Function for beta ordination
create_beta_ordination <- function(dataset, 
                                  factor_name, 
                                  measure = "bray", 
                                  method = "PCoA",
                                  plot_type = c("point", "ellipse"),
                                  title = NULL,
                                  color_values = NULL) {
  
  # Create the trans_beta object
  beta_obj <- trans_beta$new(dataset = dataset, 
                             group = factor_name, 
                             measure = measure)
  
  # Run the ordination
  beta_obj$cal_ordination(method = method)
  
  # Generate the plot title if not provided
  if(is.null(title)) {
    title <- paste("Beta Diversity by", factor_name)
  }
  
  # Create the plot
  beta_plot <- beta_obj$plot_ordination(
    plot_type = plot_type,
    plot_color = factor_name,
    plot_shape = factor_name,
  ) + 
    labs(title = title)
  
  # Return both the object and the plot
  return(list(
    beta_object = beta_obj,
    plot = beta_plot
  ))
}

# Create ordination for year
year_results <- create_beta_ordination(
  dataset = dataset, 
  factor_name = "year",
  title = "Pre-correction - Batch Effect by Year"
)

# Create ordination for batch
batch_results <- create_beta_ordination(
  dataset = dataset, 
  factor_name = "batch",
  title = "Pre-correction - Batch Effect by Sequencing Batch"
)

# Create ordination for time point
time_results <- create_beta_ordination(
  dataset = dataset, 
  factor_name = "time",
  title = "Pre-correction - Samples by Time Point"
)

# Create ordination for carriage status
carriage_results <- create_beta_ordination(
  dataset = dataset, 
  factor_name = "case_control",
  title = "Pre-correction - Samples by NM Carriage Status"
)

```

# Batch effect correction
```{r}
# Extract the abundance matrix
abundance_matrix <- dataset$otu_table
# Get the batch information
batch <- dataset$sample_table$year  # Using year as the batch variable (can be changed to 'batch' if needed)

# Apply ComBat batch correction
# First apply CLR transformation to handle compositional data

# Make sure the data is in the right format for ComBat
# ComBat expects samples as columns, features as rows, but our data might be transposed
if(nrow(abundance_matrix) > ncol(abundance_matrix)) {
  # If we have more rows than columns, the data is likely already in the right format
  # (features as rows, samples as columns)
  abundance_matrix_for_combat <- abundance_matrix
} else {
  # If we have more columns than rows, transpose
  abundance_matrix_for_combat <- t(abundance_matrix)
}

# Apply CLR transformation
# Add pseudocount to handle zeros
abundance_matrix_for_combat[abundance_matrix_for_combat == 0] <- 0.5
clr_abundance <- log2(abundance_matrix_for_combat)  # Simple log2 transform for ComBat

# Run ComBat - ensure batch is a factor
batch_factor <- as.factor(batch)
corrected_abundance <- ComBat(dat = clr_abundance, 
                              batch = batch_factor, 
                              mod = NULL,
                              par.prior = TRUE)

# Convert back from log2
corrected_counts <- 2^corrected_abundance
# Handle any negative values
corrected_counts[corrected_counts < 0] <- 0

# Ensure the corrected data is in the right format for the dataset object
if(nrow(abundance_matrix) > ncol(abundance_matrix)) {
  # If the original data had features as rows, no need to transpose back
  final_corrected_counts <- corrected_counts
} else {
  # If we transposed earlier, transpose back
  final_corrected_counts <- t(corrected_counts)
}

# Create a new dataset with corrected abundances
dataset_corrected <- dataset$clone()
dataset_corrected$otu_table <- final_corrected_counts

# Calculate beta diversity for the corrected dataset
dataset_corrected$cal_betadiv(unifrac = TRUE)

# Create post-correction ordinations using the same function
year_results_corrected <- create_beta_ordination(
  dataset = dataset_corrected, 
  factor_name = "year",
  title = "Post-correction - Batch Effect by Year"
)

batch_results_corrected <- create_beta_ordination(
  dataset = dataset_corrected, 
  factor_name = "batch",
  title = "Post-correction - Batch Effect by Sequencing Batch"
)

time_results_corrected <- create_beta_ordination(
  dataset = dataset_corrected, 
  factor_name = "time",
  title = "Post-correction - Samples by Time Point"
)

carriage_results_corrected <- create_beta_ordination(
  dataset = dataset_corrected, 
  factor_name = "case_control",
  title = "Post-correction - Samples by NM Carriage Status"
)

# Visualize all pre- and post-correction plots
# Install patchwork if not already installed
if (!requireNamespace("patchwork", quietly = TRUE)) {
  install.packages("patchwork")
}
library(patchwork)

# Compare pre- vs post-correction for each factor
year_comparison <- year_results$plot + year_results_corrected$plot
batch_comparison <- batch_results$plot + batch_results_corrected$plot
time_comparison <- time_results$plot + time_results_corrected$plot
carriage_comparison <- carriage_results$plot + carriage_results_corrected$plot

# Display the comparisons
year_comparison
batch_comparison
time_comparison
carriage_comparison

```

# Remove irrelevant saved objects
```{r}
# Remove the individual result objects
rm(batch_results)
rm(batch_results_corrected)
rm(carriage_results)
rm(carriage_results_corrected)
rm(time_results)
rm(time_results_corrected)
rm(year_results)
rm(year_results_corrected)
rm(all_plots)

# Keep the comparison plots
# batch_comparison, carriage_comparison, time_comparison, year_comparison

# Also remove intermediate data objects used for batch correction
rm(abundance_matrix)
rm(abundance_matrix_for_combat)
rm(batch)
rm(batch_factor)
rm(clr_abundance)
rm(corrected_abundance)
rm(corrected_counts)
rm(final_corrected_counts)

# Raw imported data that's now incorporated into the dataset objects
rm(feature_table)
rm(metadata)
rm(taxa)
rm(tree)

# Change the names of datasets
dataset_incorrected <- dataset
dataset <- dataset_corrected
rm(dataset_corrected)
```

# Longitudinal Carriage Status Classification
```{r}
# Step 1: Prepare data and identify carriage patterns

# Create a wide-format table showing carriage status at each time point
carriage_patterns <- dataset$sample_table %>%
  select(pid, time, case_control) %>%
  # Convert to wide format to see patterns across time
  tidyr::pivot_wider(
    id_cols = pid,
    names_from = time,
    values_from = case_control,
    names_prefix = "time_"
  )

# # Analyzing NA's:
# colSums(is.na(carriage_patterns[ , c("time_0", "time_2", "time_4")]))
# colMeans(is.na(carriage_patterns[ , c("time_0", "time_2", "time_4")])) * 100
# 
# # NA combinations
# carriage_patterns %>%
#   mutate(
#     time_0_na = !is.na(time_0),
#     time_2_na = !is.na(time_2),
#     time_4_na = !is.na(time_4),
#     na_pattern = paste(time_0_na, time_2_na, time_4_na, sep = "-")
#   ) %>%
#   count(na_pattern, name = "count") %>%
#   mutate(percent = round(count / sum(count) * 100, 1))

# Step 2: Classify each subject based on their carriage pattern with missing data awareness
carriage_patterns <- carriage_patterns %>%
  mutate(
    available_timepoints = (!is.na(time_0)) + (!is.na(time_2)) + (!is.na(time_4)),
    carriage_status = case_when(
      # Subjects with all three time points
      !is.na(time_0) & !is.na(time_2) & !is.na(time_4) & 
        time_0 == "case" & time_2 == "case" & time_4 == "case" ~ "Persistent_carrier",
      
      !is.na(time_0) & !is.na(time_2) & !is.na(time_4) & 
        time_0 == "control" & (time_2 == "case" | time_4 == "case") ~ "Acquisition_carrier",
      
      !is.na(time_0) & !is.na(time_2) & !is.na(time_4) & 
        time_0 == "case" & (time_2 == "control" | time_4 == "control") ~ "Clearance_carrier",
      
      !is.na(time_0) & !is.na(time_2) & !is.na(time_4) & 
        time_0 == "control" & time_2 == "control" & time_4 == "control" ~ "Non_carrier",
      
      # Most common pattern: time_0 and time_2 available, time_4 missing
      !is.na(time_0) & !is.na(time_2) & is.na(time_4) & 
        time_0 == "case" & time_2 == "case" ~ "Persistent_carrier_incomplete",
      
      !is.na(time_0) & !is.na(time_2) & is.na(time_4) & 
        time_0 == "control" & time_2 == "case" ~ "Acquisition_carrier_incomplete",
      
      !is.na(time_0) & !is.na(time_2) & is.na(time_4) & 
        time_0 == "case" & time_2 == "control" ~ "Clearance_carrier_incomplete",
      
      !is.na(time_0) & !is.na(time_2) & is.na(time_4) & 
        time_0 == "control" & time_2 == "control" ~ "Non_carrier_incomplete",
      
      # Pattern with missing time_2
      !is.na(time_0) & is.na(time_2) & !is.na(time_4) & 
        time_0 == "case" & time_4 == "case" ~ "Likely_persistent_carrier",
      
      !is.na(time_0) & is.na(time_2) & !is.na(time_4) & 
        time_0 == "control" & time_4 == "case" ~ "Acquisition_carrier",
      
      !is.na(time_0) & is.na(time_2) & !is.na(time_4) & 
        time_0 == "case" & time_4 == "control" ~ "Clearance_carrier",
      
      !is.na(time_0) & is.na(time_2) & !is.na(time_4) & 
        time_0 == "control" & time_4 == "control" ~ "Non_carrier",
      
      # Missing baseline but have other timepoints
      is.na(time_0) & !is.na(time_2) & (!is.na(time_4)) ~ "Insufficient_baseline_data",
      
      # Insufficient data (only one timepoint)
      available_timepoints == 1 ~ "Insufficient_data",
      
      # Catch-all for any other patterns
      TRUE ~ "Undetermined"
    )
  )

# Step 3: Examine the distribution of carriage status categories
table(carriage_patterns$carriage_status)

# Create a simplified categorization for analysis
carriage_patterns <- carriage_patterns %>%
  mutate(
    carriage_group = case_when(
      grepl("Persistent", carriage_status) ~ "Persistent_carrier",
      grepl("Acquisition", carriage_status) ~ "Acquisition_carrier",
      grepl("Clearance", carriage_status) ~ "Clearance_carrier",
      grepl("Non_carrier", carriage_status) ~ "Non_carrier",
      TRUE ~ "Insufficient_data"
    )
  )

# Check distribution of simplified groups
table(carriage_patterns$carriage_group)

# Step 4: Add carriage status and group back to the sample table
# First create a lookup table with pid and carriage_status/group
carriage_lookup <- carriage_patterns %>%
  select(pid, carriage_status, carriage_group)

# Then join this information back to your sample data
dataset$sample_table <- dataset$sample_table %>%
  left_join(carriage_lookup, by = "pid")

# Verify the data structure
head(dataset$sample_table)

rm(carriage_lookup)
```

##modeling with ANCOM-BC2
#library loads an preparing data
```{r}
library(readxl)       # For reading Excel files
library(dplyr)        # For data manipulation
library(ANCOMBC)      # For ANCOM-BC2 analysis
library(phyloseq)     # For microbiome data handling
library(ggplot2)      # For visualization
library(tidyr)        # For data reshaping
library(DT)  

# Import demographic data
demographic_data <- read_excel("demographic data _Jan2022_final.YM.20241230.xlsx") #demographic data *Jan2022*final.YM.20241230.xlsx

# Clean and prepare demographic data (factor important variables we want to check)
demographic_data <- demographic_data %>%
  mutate(
    pid = as.character(pid),  # Ensure pid is character for joining
    boardingschool = factor(boardingschool),
    selfsmoking = factor(selfsmoking),
    secondsmoker = factor(secondsmoker),
    AB_month = factor(AB_month),  # Antibiotic use in previous month
    AB_year = factor(AB_year)     # Antibiotic use in previous year
  )

# Look at the structure of the cleaned demographic data
#str(demographic_data)

# Merge demographic data with sample table (only relevant variables, note - no extandent Medical History and Conditions and Sampling and Laboratory Results)
dataset$sample_table <- dataset$sample_table %>%
  left_join(demographic_data %>% 
              select(pid, boardingschool, selfsmoking, secondsmoker, 
                     AB_month, AB_year, yearofbirth, family_members), 
            by = "pid")

# Ensure time is treated as a factor for modeling
dataset$sample_table$time <- factor(dataset$sample_table$time)

# Convert to phyloseq object for ANCOMBC
otu_table <- dataset$otu_table
tax_table <- as.matrix(dataset$tax_table)
rownames(dataset$sample_table) <- dataset$sample_table[,1]
sample_data <- dataset$sample_table

ps <- phyloseq(
  otu_table(otu_table, taxa_are_rows = TRUE),
  tax_table(tax_table),
  sample_data(sample_data)
)

# Check the number of samples in each carriage group
table(sample_data$carriage_group, sample_data$time)
```

# SUSCEPTIBILITY ANALYSIS: Expanding the aquisition carriers group
```{r}
# 1. Identify subjects showing NM acquisition patterns by examining carriage patterns
# This includes both traditional acquisition and reacquisition patterns

# Start with the carriage patterns you already created
expanded_carriage_patterns <- carriage_patterns %>%
  mutate(
    # Define expanded acquisition status
    shows_acquisition = case_when(
      # Traditional acquisition (control → case)
      !is.na(time_0) & time_0 == "control" & 
        ((!is.na(time_2) & time_2 == "case") | (!is.na(time_4) & time_4 == "case")) ~ TRUE,
      
      # Reacquisition (case → control → case)
      !is.na(time_0) & !is.na(time_2) & !is.na(time_4) & 
        time_0 == "case" & time_2 == "control" & time_4 == "case" ~ TRUE,
      
      # Alternatively, capture pattern with missing middle time point
      !is.na(time_0) & is.na(time_2) & !is.na(time_4) & 
        time_0 == "case" & time_4 == "case" & grepl("Clearance", carriage_status) ~ TRUE,
      
      # Default
      TRUE ~ FALSE
    )
  )

# Get list of PIDs showing acquisition
acquisition_pids <- expanded_carriage_patterns %>%
  filter(shows_acquisition == TRUE) %>%
  pull(pid)

# Print information about the expanded group
cat("Number of subjects in expanded acquisition group:", length(acquisition_pids), "\n")

# 2. Create a new column in the sample table for the expanded grouping
dataset$sample_table <- dataset$sample_table %>%
  mutate(
    susceptibility_group = case_when(
      pid %in% acquisition_pids ~ "Ever_acquired_NM",
      carriage_group == "Non_carrier" ~ "Never_acquired_NM",
      TRUE ~ "Other"  # All other patterns
    )
  )

# Check the distribution of the new grouping
print(table(dataset$sample_table$susceptibility_group, dataset$sample_table$time))

# 3. Update the phyloseq object with the new grouping
sample_data <- dataset$sample_table
rownames(sample_data) <- sample_data$SampleID  # Ensure rownames are set correctly

ps_updated <- phyloseq(
  otu_table(otu_table, taxa_are_rows = TRUE),
  tax_table(tax_table),
  sample_data(sample_data)
)

# 4. Create subset for expanded susceptibility analysis
ps_baseline <- subset_samples(ps_updated, time == "0")
ps_suscept_expanded <- subset_samples(ps_baseline, 
                                     susceptibility_group %in% c("Ever_acquired_NM", "Never_acquired_NM"))

# Check sample sizes for the expanded analysis
print("Sample counts for expanded susceptibility analysis:")
print(table(sample_data(ps_suscept_expanded)$susceptibility_group))

# 5. Run ANCOM-BC2 with expanded susceptibility grouping
set.seed(123)  # For reproducibility
ancombc2_suscept_expanded <- ancombc2(
  data = ps_suscept_expanded,
  tax_level = "Genus",
  fix_formula = "susceptibility_group + selfsmoking + boardingschool + AB_month",
  rand_formula = NULL,
  p_adj_method = "holm",
  pseudo_sens = TRUE,
  prv_cut = 0.025,
  s0_perc = 0.1,
  group = "susceptibility_group",
  struc_zero = TRUE,
  neg_lb = TRUE,
  alpha = 0.05,
  n_cl = 2,
  verbose = TRUE,
  global = FALSE,
  pairwise = TRUE,
  dunnet = FALSE,
  trend = FALSE,
  iter_control = list(tol = 1e-5, max_iter = 20, verbose = FALSE),
  em_control = list(tol = 1e-5, max_iter = 100),
  lme_control = NULL,
  mdfdr_control = list(fwer_ctrl_method = "holm", B = 100)
)

# 6. Save the results
saveRDS(ancombc2_suscept_expanded, "ancombc2_expanded_susceptibility_results.rds")

# 7. Function to visualize ANCOMBC2 results (same as you had in your original code)
visualize_ancombc2_results <- function(ancombc2_result, title) {
  # Extract pairwise results if they exist
  res_primary <- ancombc2_result$res
  
  # Create empty list for plots and significant taxa
  plot_list <- list()
  sig_taxa_list <- list()
  
  # Get the column names related to log fold changes and differential expression
  lfc_cols <- grep("^lfc_", colnames(res_primary), value = TRUE)
  diff_cols <- grep("^diff_", colnames(res_primary), value = TRUE)
  
  # Process each comparison
  for (i in seq_along(lfc_cols)) {
    # Get corresponding column names
    lfc_col <- lfc_cols[i]
    diff_col <- diff_cols[i]
    
    # Skip if not in the data
    if (!lfc_col %in% colnames(res_primary) || !diff_col %in% colnames(res_primary)) {
      next
    }
    
    # Extract the comparison name from the column
    comparison <- gsub("^lfc_", "", lfc_col)
    
    # Create data frame for plotting
    plot_data <- data.frame(
      taxon = res_primary$taxon,
      lfc = res_primary[[lfc_col]],
      diff = res_primary[[diff_col]]
    ) %>%
      # Filter for significant taxa
      filter(diff == 1) %>%
      # Sort by absolute log fold change
      arrange(desc(abs(lfc)))
    
    # Skip if no significant taxa
    if (nrow(plot_data) == 0) {
      next
    }
    
    # Create bar plot of significant taxa
    # Limit to top 20 for visibility
    top_n_taxa <- min(nrow(plot_data), 20)
    bar_data <- plot_data[1:top_n_taxa, ]
    
    # Create bar plot
    bar_plot <- ggplot(bar_data, aes(x = reorder(taxon, lfc), y = lfc, 
                                    fill = lfc > 0)) +
      geom_bar(stat = "identity") +
      scale_fill_manual(values = c("#619CFF", "#F8766D"), 
                      name = "Direction", 
                      labels = c("Decreased", "Increased")) +
      labs(
        title = paste0(title, " - ", comparison),
        x = "Taxon",
        y = "Log Fold Change"
      ) +
      coord_flip() +
      theme_minimal()
    
    # Add plot to list
    plot_list[[comparison]] <- bar_plot
    
    # Add significant taxa to list
    sig_taxa_list[[comparison]] <- plot_data
  }
  
  return(list(plots = plot_list, sig_taxa = sig_taxa_list))
}

# 8. Process results from expanded analysis
suscept_expanded_vis <- visualize_ancombc2_results(ancombc2_suscept_expanded, 
                                                 "Expanded Susceptibility Analysis")

# 9. Display the results
display_results <- function(vis_results, title) {
  cat("\n====== ", title, " ======\n")
  
  # Check if there are significant results
  if (length(vis_results$sig_taxa) == 0) {
    cat("No significant differential abundance detected.\n")
    return(NULL)
  }
  
  # Display results for each comparison
  for (comparison in names(vis_results$sig_taxa)) {
    sig_taxa <- vis_results$sig_taxa[[comparison]]
    
    if (nrow(sig_taxa) > 0) {
      cat("\nComparison:", comparison, "\n")
      cat("Number of significant taxa:", nrow(sig_taxa), "\n")
      
      # Display top 10 taxa by absolute log fold change
      top_taxa <- head(sig_taxa, 10)
      print(top_taxa %>% select(taxon, lfc))
      
      # Display the plot in interactive environments
      if (interactive()) {
        print(vis_results$plots[[comparison]])
      }
    }
  }
}

# Display results from expanded analysis
display_results(suscept_expanded_vis, "SIGNIFICANT TAXA FOR EXPANDED SUSCEPTIBILITY ANALYSIS")
```

# RESILIENCE ANALYSIS: Comparing Persistent Carriers vs Clearance Carriers
```{r}
# This analysis looks for microbial taxa that differentiate those who clear NM vs those who remain carriers

# 1. Create subset for Persistent vs Clearance analysis at baseline (time 0)
# Start with baseline samples as this represents the initial microbiome state
ps_baseline <- subset_samples(ps_updated, time == "0")
ps_persist_vs_clear <- subset_samples(ps_baseline, 
                                     carriage_group %in% c("Persistent_carrier", "Clearance_carrier"))

# Check sample sizes for analysis
print("Sample counts for persistent vs clearance carriers at baseline:")
print(table(sample_data(ps_persist_vs_clear)$carriage_group))

# 2. Run ANCOM-BC2 to identify differentially abundant taxa
set.seed(123)  # For reproducibility
ancombc2_persist_vs_clear <- ancombc2(
  data = ps_persist_vs_clear,
  tax_level = "Genus",
  fix_formula = "carriage_group + selfsmoking + boardingschool + AB_month",
  rand_formula = NULL,
  p_adj_method = "holm",
  pseudo_sens = TRUE,
  prv_cut = 0.025,
  s0_perc = 0.1,
  group = "carriage_group",
  struc_zero = TRUE,
  neg_lb = TRUE,
  alpha = 0.05,
  n_cl = 2,
  verbose = TRUE,
  global = FALSE,
  pairwise = TRUE,
  dunnet = FALSE,
  trend = FALSE,
  iter_control = list(tol = 1e-5, max_iter = 20, verbose = FALSE),
  em_control = list(tol = 1e-5, max_iter = 100),
  lme_control = NULL,
  mdfdr_control = list(fwer_ctrl_method = "holm", B = 100)
)

# 3. Save the results
saveRDS(ancombc2_persist_vs_clear, "ancombc2_persistent_vs_clearance_results.rds")

# 4. Visualize results using the existing function
persist_vs_clear_vis <- visualize_ancombc2_results(ancombc2_persist_vs_clear, 
                                                 "Persistent vs Clearance Carriers")

# 5. Display the results 
display_results(persist_vs_clear_vis, "SIGNIFICANT TAXA FOR PERSISTENT VS CLEARANCE CARRIERS")

# 6. Optional: Create a heatmap of significant taxa
# Extract significant taxa from the results
if(length(persist_vs_clear_vis$sig_taxa) > 0) {
  sig_taxa_names <- unique(unlist(lapply(persist_vs_clear_vis$sig_taxa, function(x) x$taxon)))
  
  if(length(sig_taxa_names) > 0) {
    # Extract abundance data for these taxa
    sig_taxa_idx <- match(sig_taxa_names, taxa_names(ps_persist_vs_clear))
    sig_taxa_idx <- sig_taxa_idx[!is.na(sig_taxa_idx)]
    
    if(length(sig_taxa_idx) > 0) {
      # Extract and transform abundance data
      sig_abund <- log10(otu_table(ps_persist_vs_clear)[sig_taxa_idx,] + 1)
      
      # Create a heatmap
      heatmap_data <- as.data.frame(t(sig_abund))
      heatmap_data$SampleID <- rownames(heatmap_data)
      heatmap_data$CarriageGroup <- sample_data(ps_persist_vs_clear)$carriage_group
      
      # Reshape to long format for ggplot
      heatmap_long <- heatmap_data %>%
        tidyr::pivot_longer(cols = -c(SampleID, CarriageGroup),
                           names_to = "Taxon", 
                           values_to = "Abundance")
      
      # Create heatmap
      heatmap_plot <- ggplot(heatmap_long, aes(x = SampleID, y = Taxon, fill = Abundance)) +
        geom_tile() +
        facet_grid(. ~ CarriageGroup, scales = "free_x", space = "free_x") +
        scale_fill_viridis_c(name = "Log10\nAbundance") +
        theme_minimal() +
        theme(axis.text.x = element_blank(),
              axis.ticks.x = element_blank()) +
        labs(title = "Abundance of Significant Taxa by Carriage Group",
             x = "Sample", y = "Taxon")
      
      print(heatmap_plot)
    }
  }
}
```

